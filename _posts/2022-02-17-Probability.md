---
layout: post
title: æ¦‚ç‡è®º 1.0
categories: BS-NJU-Course-Review-Physics
description: æ¦‚ç‡è®º
keywords: probability
mathjax: true
---

ç”¨æ¦‚ç‡æè¿°çš„ç›®çš„æ¥æºäºå¯¹äº‹å®çš„ä¸ç¡®å®šæ€§ã€‚ç†è®ºä¸Šï¼Œå¯ä»¥æ ¹æ®æˆ‘ä»¬å¯¹äº‹å®ï¼ˆå¹¶ä¸å®Œå…¨çš„è®¤è¯†çš„ï¼‰çŸ¥è¯†ç»™å‡ºä¸€ä»¶äº‹å‘ç”Ÿçš„ï¼ˆå¯èƒ½å¹¶ä¸å‡†ç¡®çš„ï¼‰æ¦‚ç‡ï¼Œç”¨äº**æƒè¡¡åˆ©å¼Š**ã€‚è¿™ä¸ªæ¦‚ç‡çš„çœŸå®æ€§éœ€è¦é€šè¿‡è¿™ä¸ªäº‹ä»¶å®é™…å‘ç”Ÿçš„é¢‘ç‡æ ¸å®ã€‚å½“æˆ‘ä»¬å…³äºäº‹å®çš„çŸ¥è¯†æ”¹å˜æ—¶ï¼Œå¯èƒ½è¿™ä¸ªç†è®ºä¸Šçš„æ¦‚ç‡å€¼ä¹Ÿå°†æ”¹å˜ã€‚åœ¨ [(En) Machine Learning](https://shi200005.github.io/2022/12/05/Machine-Learning/) å’Œä¹‹åçš„ [(En) Information Theory](https://shi200005.github.io/2023/10/03/Information-Theory/) ä¸­ï¼Œå°†ä¼šæœ‰æ›´æ·±çš„ä½“ä¼šã€‚

æœ¬æƒ³å¤ä¹ å®Œå¾®ç§¯åˆ†ä¸çº¿æ€§ä»£æ•°åå…ˆå¼€å§‹å¤ä¹ åŠ›å­¦ä¸çƒ­å­¦ï¼Œä¸è¿‡é¢„è§åˆ°ä¸ä¹…åçš„å®è·µä¸­ä¼šç”¨åˆ°æ¦‚ç‡è®ºï¼Œä»¥åŠå…¶å®æš‘æ ¡çš„ï¼ˆå°è¯•ï¼‰ä¸»å…ƒåˆ†æå®è·µä¸­å·²ç»ç”¨åˆ°æ¦‚ç‡è®ºï¼Œæ‰€ä»¥æˆ–è®¸è¯¥æ—©ç‚¹å¤ä¹ ã€‚å…³äºç›¸å…³çŸ¥è¯†çš„é‡è¦æ€§ï¼Œæˆ‘å¯ä»¥è´´ä¸€ä¸ªæœ‰è¶£çš„TEDè§†é¢‘ï¼Œéå¸¸ç®€çŸ­ï¼Œæ— å¹²è´§ï¼Œæœ‰è¶£è€Œå·²ã€‚[TEDæ¼”è®²ï¼šå…ˆæ•™ç»Ÿè®¡å­¦å†æ•™å¾®ç§¯åˆ†!](https://www.bilibili.com/video/BV1W7411E7JF?p=2)

2024.04.29 æ›´æ–°ï¼šç›¸å…³ç§‘æ™®ä¹¦ï¼š*The Drunkard's Walk*: How Randomness Rules Our Lives.

äº‹å®ä¸Šï¼Œè¿™é—¨è¯¾å¯èƒ½æ˜¯æœ€ä¸ºé‡è¦çš„ä¸€é—¨è¯¾ï¼ˆå°¤å…¶æ˜¯æ—¥åè½¬åˆ°å…¶ä»–å®šé‡ç§‘å­¦é¢†åŸŸï¼‰ã€‚åŒæ—¶ï¼Œä½œä¸º9-11èŠ‚çš„æ™šé—´ä¸“ä¸šé€‰ä¿®è¯¾ï¼Œè¿™é—¨è¯¾ä¹Ÿæ˜¯ä¸€é—¨ç»å¯¹çš„æ°´è¯¾ã€‚å¦‚æœæˆ‘æ—©ç‚¹æ„è¯†åˆ°è¿™é—¨è¯¾å¯¹æˆ‘çš„é‡è¦æ€§å’Œä¿®è¯»è¯¾ç¨‹çš„æ°´æ€§ï¼Œæ—©ç‚¹å¼€å§‹è‡ªå­¦ï¼Œå°±ä¸ä¼šèµ°è¿™ä¹ˆå¤šå¼¯è·¯äº†ã€‚

æ›´æ–°è®°å½• 2021.10.02 Ver0.0, 2021.11.09 Ver0.1, 2021.11.17 Ver0.2, 2022.11.12 Ver1.0, 2022.11.23 Ver1.1, 2023.10.14 Ver1.2 (law of weak large number, generating functions)

| å­¦ä¹ æ—¶é—´                                   | å¤§äºŒä¸Š                                   |
| å‘¨å­¦æ—¶                                       | 3                                             |
| æœ¬äººæˆç»©                                   | 94                                           |
| è¯¾ç¨‹æ•™æ                                   | åå­˜å®äº¡ å·²ç»ç‰ˆ...                |
| ä¸ªäººå»ºè®®å‚è€ƒæ•™æ                   | æœ¬äººè¿™ç¯‡                                |
| å…ˆä¿®è¯¾ç¨‹                                   | å¾®ç§¯åˆ†  çº¿æ€§ä»£æ•°  å¤å˜å‡½æ•° |

## External References

1. Kardar, Mehran. *Statistical physics of particles*. Cambridge University Press, 2007.
2. Van Kampen, Nicolaas Godfried. *Stochastic processes in physics and chemistry*. Vol. 1. Elsevier, 1992.

## æ¦‚ç‡è®ºçš„åŸºæœ¬æ¦‚å¿µ

### ç­‰å¯èƒ½æ¦‚å‹ï¼ˆå¤å…¸æ¦‚å‹ï¼‰

### æ¡ä»¶æ¦‚ç‡

$$\displaystyle P(A|B)=\frac{P(A,B)}{P(B)}$$ å¾ˆé‡è¦ï¼
### å…¨æ¦‚ç‡å…¬å¼ä¸Bayes å…¬å¼

1. $$\displaystyle P(B_i|A)=\frac{P(A|B_i)P(B_i)}{\displaystyle\sum_{j=1}^{n}P(A|B_j)P(B_j)}$$

2. å¯ç”¨äºæ±‚è§£**åéªŒæ¦‚ç‡**ï¼ˆåœ¨å¾—åˆ°ä¿¡æ¯ä¹‹åå†é‡æ–°åŠ ä»¥ä¿®æ­£çš„æ¦‚ç‡ï¼‰ï¼Œåˆ©ç”¨**æ¡ä»¶æ¦‚ç‡**å…¬å¼è”ç³»**åéªŒæ¦‚ç‡**å’Œ**å…ˆéªŒæ¦‚ç‡**
   
      å…ˆéªŒæ¦‚ç‡ï¼šå·²çŸ¥çš„ä¿¡æ¯ï¼›åéªŒæ¦‚ç‡ï¼šç»™å·²çŸ¥ä¿¡æ¯åå¯¹è®¤çŸ¥è¿›è¡Œä¿®æ­£ã€‚ã€Intuitionã€‘æµ‹è¯•ç»“æœä¸ºé˜³æ€§çš„äººæ˜¯æ„ŸæŸ“è€…çš„æ¦‚ç‡ <-> æ„ŸæŸ“è€…çš„æ£€æµ‹ç»“æœæ˜¯é˜³æ€§çš„æ¦‚ç‡ã€‚
      
      ã€GRAD-UPDATEã€‘åœ¨ [(En) Machine Learning ](https://shi200005.github.io/2022/12/05/Machine-Learning/)ç›¸å…³ç†è®ºçš„é‡è¦èƒŒæ™¯ã€‚

### ç‹¬ç«‹äº‹ä»¶

è‹¥ $$A, B$$ äº’ä¸ºç‹¬ç«‹äº‹ä»¶ï¼Œåˆ™ $$P(A,B)=P(A)P(B)$$, $$P(A\vert B)=P(A)$$. 

## éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ

### ç¦»æ•£å‹éšæœºå˜é‡

#### äºŒé¡¹åˆ†å¸ƒ 

$$X\sim b(n,p)$$

$$n$$ é‡ä¼¯åŠªåˆ©è¯•éªŒ -> äºŒé¡¹åˆ†å¸ƒ

#### æ³Šæ¾åˆ†å¸ƒ 

$$X\sim Ï€(Î»)$$

æ³Šæ¾åˆ†å¸ƒä½œä¸ºæ³Šæ¾è¿‡ç¨‹çš„â€œæŸæ—¶æ®µå†…å‘ç”Ÿäº‹ä»¶æ•°â€çš„åˆ†å¸ƒã€‚æ³Šæ¾è¿‡ç¨‹ä¸­äº‹ä»¶å‘ç”Ÿçš„ç­‰å¾…æ—¶é—´é—´éš”æœä»æŒ‡æ•°åˆ†å¸ƒ(è¯¦è§â€œè¿ç»­å‹éšæœºå˜é‡--æŒ‡æ•°åˆ†å¸ƒâ€)ã€‚[æ³Šæ¾åˆ†å¸ƒä¸æŒ‡æ•°åˆ†å¸ƒçš„è”ç³»](https://zhuanlan.zhihu.com/p/261961315)ã€‚ã€SELF-STUDYã€‘æ³Šæ¾åˆ†å¸ƒå¯ä»¥ä»éå¸¸å°‘çš„å‡è®¾ï¼ˆ**ç‹¬ç«‹å‘ç”Ÿ**ã€**æ— èšæŸæ•ˆåº”**ï¼‰æ¨å¯¼å‡ºæ¥ã€‚æœ¬äººçŸ¥ä¹æ–‡ç« ï¼ˆæ¬è¿ã€Šæ•°å­¦åŠ¨åŠ›å­¦æ¨¡å‹ã€‹ï¼‰[æ³Šæ¾åˆ†å¸ƒä¸æ³Šæ¾è¿‡ç¨‹](https://zhuanlan.zhihu.com/p/431389631)ã€‚

æ³Šæ¾åˆ†å¸ƒä½œä¸ºäºŒé¡¹åˆ†å¸ƒçš„è¿‘ä¼¼ï¼š$$n$$ å¾ˆå¤§è€Œ $$p$$ å¾ˆå° -> $$X\sim b(n,p)$$-> $$X\sim Ï€(Î»)$$, $$Î»=np$$.ï¼ˆ$$n$$ å¾ˆå¤§çš„è¯ï¼Œç®—é˜¶ä¹˜ç¨‹åºè‚¯å®šä¼šæ­»ï¼‰ã€‚è¯æ˜å¾ˆç®€å•ï¼Œè¯¦è§[æ¦‚ç‡è®º-æ³Šæ¾åˆ†å¸ƒä½œä¸ºäºŒé¡¹åˆ†å¸ƒçš„è¿‘ä¼¼](https://shi200005.github.io/download_file/Probability_Binomial_Poisson.pdf)ã€‚Intuition: ç”¨æ³Šæ¾è¿‡ç¨‹æ¨å¯¼æ³Šæ¾åˆ†å¸ƒçš„æ—¶å€™ï¼Œå°±æ˜¯ä»ä¸€ä»¶äº‹å‘ç”Ÿè¿˜æ˜¯ä¸å‘ç”Ÿå‡ºå‘ï¼Œå–æ—¶é—´é—´éš”çš„æé™å°ï¼Œä¹Ÿå°±æ˜¯è€ƒè™‘æé™æƒ…å†µä¸‹åšä¼¯åŠªåˆ©è¯•éªŒæˆä¸æˆåŠŸï¼Œæ‰€ä»¥æ˜¯äºŒé¡¹åˆ†å¸ƒçš„æé™æƒ…å†µã€‚

ã€GRAD-UPDATEã€‘ä»ä¸€ä¸ªæ³Šæ¾è¿‡ç¨‹ä¸­éšæœºé€‰å–ä¸€äº›äº‹ä»¶ï¼ˆé€‰å–çš„æ¦‚ç‡æ˜¯å¸¸æ•° $$r$$ï¼‰ï¼Œè¿™äº›äº‹ä»¶ä¾ç„¶æ„æˆæ³Šæ¾è¿‡ç¨‹ã€‚

ã€GRAD-UPDATEã€‘æ³Šæ¾è¿‡ç¨‹çš„é€‚ç”¨æ¡ä»¶ï¼šè¯•æƒ³ä¸€ä¸ªä»»æ„çš„è¿‡ç¨‹ï¼Œä»è¯¥è¿‡ç¨‹ä¸­éšæœºåœ°é€‰å–æå°‘çš„äº‹ä»¶ã€‚äº‹ä»¶ä¹‹é—´çš„ç›¸å…³æ€§éšæ—¶é—´è¡°å‡ï¼Œå¦‚æœé€‰çš„äº‹ä»¶è¶³å¤Ÿç¨€å°‘ï¼Œé‚£ä¹ˆé€‰å‡ºçš„äº‹ä»¶å¯ä»¥è¿‘ä¼¼åœ°ç”¨æ³Šæ¾è¿‡ç¨‹æè¿°ã€‚

ã€GRAD-UPDATEã€‘æœ¬ç§‘é˜¶æ®µåªè®¨è®ºäº†äº‹ä»¶å‘ç”Ÿæ¦‚ç‡æ˜¯å¸¸æ•° $$r$$ çš„æƒ…å†µï¼Œç»“è®ºæ˜¯ä¸€æ®µæ—¶é—´ $$T$$ å†…å‘ç”Ÿäº‹ä»¶çš„è®¡æ•° $$N$$ æ»¡è¶³ $$\langle N\rangle=rt$$ï¼Œ$$\langle N^2\rangle=\langle N\rangle$$. è€ƒè™‘æ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œäº‹ä»¶å‘ç”Ÿæ¦‚ç‡ $$r(t)$$ éšæ—¶é—´å˜åŒ–ï¼Œä½†æ˜¯å„ä¸ªäº‹ä»¶çš„å‘ç”Ÿæ²¡æœ‰å…³è”ï¼Œæœ‰ç»“è®º $$\displaystyle\langle N\rangle=\int_0^Tdt r(t)$$ï¼Œ$$\langle N^2\rangle=\langle N\rangle$$ã€‚æ–‡ç« è¯¦è§[æ³Šæ¾åˆ†å¸ƒä¸æ³Šæ¾åˆ†å¸ƒ(II)](https://zhuanlan.zhihu.com/p/603123138)ï¼Œæ¨å¯¼è¿‡ç¨‹è¯¦è§[æ¦‚ç‡è®º-ä¸€èˆ¬çš„æ³Šæ¾è¿‡ç¨‹ä¸æ³Šæ¾åˆ†å¸ƒ](https://shi200005.github.io/download_file/Pobability_Poisson_Inhomo.pdf)ã€‚

ã€GRAD-UPDATEã€‘**ä¸€ç»´éšæœºæ¸¸èµ°**ï¼ˆbirth-and-deathï¼‰è¿‡ç¨‹ä¸­ï¼Œç»†è‡´å¹³è¡¡ç»™å‡ºç¨³æ€æ—¶ç³»ç»Ÿåœ¨ç¨³æ€â€œæ ¼ç‚¹â€é™„è¿‘çš„åˆ†å¸ƒä¹Ÿæ˜¯æ³Šæ¾åˆ†å¸ƒã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸º bounded æ³Šæ¾è¿‡ç¨‹ç»™å‡ºæ³Šæ¾åˆ†å¸ƒï¼ˆè¯¦è§æˆ‘çš„åšå£«å­¦ä½è®ºæ–‡ï¼Ÿï¼‰

#### å‡ ä½•åˆ†å¸ƒ

ã€GRAD-UPDATEã€‘åœ¨ä¼¯åŠªåˆ©å®éªŒä¸­ï¼Œå¾—åˆ°ä¸€æ¬¡æˆåŠŸæ‰€éœ€è¦çš„è¯•éªŒæ¬¡æ•°åˆ†å¸ƒã€‚

åœ¨ç”Ÿç‰©ç‰©ç†å®éªŒæ•°æ®ä¸­æè¿°å¤šæ€åŠ¨åŠ›å­¦çš„ burst-size distributionã€‚

### è¿ç»­å‹éšæœºå˜é‡ 

åˆ†å¸ƒå‡½æ•° c.d.f.ï¼ˆæ•°å­¦å®¶æ›´å–œæ¬¢ï¼Œè€ƒè™‘åˆ°æœ‰ç¦»æ•£æƒ…å†µçš„æ•°å­¦å½¢å¼ï¼‰ã€æ¦‚ç‡å¯†åº¦ p.d.f.ï¼ˆç‰©ç†å­¦å®¶æ›´å–œæ¬¢ï¼Œä¸ç»Ÿè®¡ç³»ç»¼çš„æ¦‚å¿µè”ç³»ç´§å¯†ï¼Œç›¸ä¿¡ä¿¡æ¯å­¦å®¶ä¹Ÿæ›´å–œæ¬¢è¿™ä¸ªï¼‰ã€‚

#### å‡åŒ€åˆ†å¸ƒ 

$$X\sim U(a,b)$$

#### æŒ‡æ•°åˆ†å¸ƒ

æŒ‡æ•°åˆ†å¸ƒå…·æœ‰æ— è®°å¿†æ€§ã€‚é—²èŠï¼šçŸ¥ä¹ç½‘å‹[æŒ‡æ•°åˆ†å¸ƒçš„åˆ†å¸ƒå‡½æ•°æ˜¯æ€ä¹ˆå¾—åˆ°çš„ï¼Ÿ](https://www.zhihu.com/question/354825596/answer/893242882)ã€‚

æ³Šæ¾åˆ†å¸ƒä¸æŒ‡æ•°åˆ†å¸ƒçš„è”ç³»ï¼šç¬¦åˆæ³Šæ¾åˆ†å¸ƒçš„ä¸¤ä¸ªäº‹ä»¶å‘ç”Ÿæ—¶é—´é—´éš”ç¬¦åˆæŒ‡æ•°åˆ†å¸ƒã€‚

#### æ­£æ€åˆ†å¸ƒï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰

ï¼ˆ$$X$$~$$N(Î¼,Ïƒ^2)$$ï¼‰

æ­£æ€åˆ†å¸ƒä½œä¸ºäºŒé¡¹åˆ†å¸ƒçš„è¿‘ä¼¼ï¼š$$n$$ å¾ˆå¤§ï¼Œ$$p$$ æ¥è¿‘ $$1/2$$ -> $$X\sim b(n,p)$$ ->  $$X\sim N(Î¼,Ïƒ^2)$$ã€‚ã€GRAD-UPDATEã€‘ç»“è®ºæ¥è‡ªäºæ±‚è§£ Fokker-Planck è¿‘ä¼¼ä¸‹çš„å°åå·®ï¼ˆ$$p$$ æ¥è¿‘äº $$1/2$$ï¼‰çš„æ¦‚ç‡åå¾®åˆ†æ–¹ç¨‹ï¼Œéœ€è¦ç”¨åˆ°å‚…é‡Œå¶å˜æ¢ï¼Œè¯¦è§[éšæœºè¿‡ç¨‹](https://shi200005.github.io/2022/10/28/Stochastic-Processes/)ã€‚

çŸ¥ä¹ç½‘å‹å…³äºæ³Šæ¾åˆ†å¸ƒä¸æ­£æ€åˆ†å¸ƒä½œä¸ºäºŒé¡¹åˆ†å¸ƒçš„è¿‘ä¼¼çš„å®šé‡åˆ»ç”»å·¥ä½œ[å®šé‡ç†è§£äºŒé¡¹å¼åˆ†å¸ƒçš„æ³Šæ¾å’Œé«˜æ–¯è¿‘ä¼¼](https://zhuanlan.zhihu.com/p/27604254)ã€‚

å…³äºæ›´å¤šéšæœºå˜é‡åˆ†å¸ƒ[æ¦‚ç‡åˆ†å¸ƒ](https://www.yuque.com/angsweet/machine-learning/shu-xue-ji-chu_shu-xue-ji-chu_gai-lv-tong-ji_gai-lv-fen-bu)ã€‚[ä¸€å¼ å›¾è¯´æ˜äºŒé¡¹åˆ†å¸ƒã€æ³Šæ¾åˆ†å¸ƒã€æŒ‡æ•°åˆ†å¸ƒã€å‡ ä½•åˆ†å¸ƒã€è´ŸäºŒé¡¹åˆ†å¸ƒã€ä¼½ç›åˆ†å¸ƒçš„è”ç³»](https://zhuanlan.zhihu.com/p/32932782)ã€‚

### éšæœºå˜é‡å’Œçš„åˆ†å¸ƒ

å¦‚æœéšæœºå˜é‡ $$X_1\sim P_1$$ å’Œ $$X_2\sim P_2$$ ç›¸äº’ **ç‹¬ç«‹**ï¼Œåˆ™ $$X=X_1+X_2$$ çš„åˆ†å¸ƒå‡½æ•°ä¸º 

è¿ç»­ï¼š$$P(X=x)=\displaystyle\int_{all} P_1(X_1=u)P_2(X_2=X-u)du$$

ç¦»æ•£ï¼š$$P(X=x)=\displaystyle\sum_{all} P_1(X_1=u)P_2(X_2=X-u)$$

æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°äº†**å·ç§¯**ï¼Œäºæ˜¯æƒ³åˆ°äº†**å‚…é‡Œå¶å˜æ¢**çš„**å·ç§¯å®šç†**ï¼ˆè¯¦è§[å¤å˜å‡½æ•°](https://shi200005.github.io/2022/02/15/Complex-Functions/#%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2)ï¼‰ã€‚

ç»“è®º

- æ³Šæ¾ + æ³Šæ¾è¿˜æ˜¯æ³Šæ¾ï¼Œå‚æ•°ä¸ºæ±‚å’Œã€‚

- å¤šä¸ªç›¸åŒæŒ‡æ•°åˆ†å¸ƒéšæœºå˜é‡çš„å’Œ

  ã€GRAD-UPDATEã€‘Erlang Distribution: äº§ç”Ÿäºç”µè¯æ¥çº¿èƒŒæ™¯ï¼Œå¯ä»¥ç”¨äºæè¿°ç”Ÿç‰©ä¿¡å·è½¬å¯¼ä¸­çš„ N-Step Cascadeï¼Œæ¯ä¸€æ­¥éƒ½æ˜¯æŒ‡æ•°åˆ†å¸ƒã€‚æŒ‡æ•°åˆ†å¸ƒåœ¨å‡å€¼å¤„æ²¡æœ‰å³°å€¼ï¼Œæ˜¯ä¸ªä¹±ä¸ƒå…«ç³Ÿçš„åˆ†å¸ƒï¼Œä½†æ˜¯å¦‚æœå¤šæ­¥çº§è”ï¼ˆå‡ ä¸ªéšæœºå˜é‡çš„å’Œï¼‰ï¼Œå°±ä¸é‚£ä¹ˆä¹±ä¸ƒå…«ç³Ÿäº†ã€‚

  Example in biophysics: analytical continuous model solution of protein distribution when produced in bursts, and the burst size obeys the exponential distribution. Reference: Friedman, N., Cai, L., & Xie, X. S. (2006). Linking stochastic  dynamics to population distribution: an analytical framework of gene  expression. *Physical review letters*, *97*(16), 168302. [online](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.97.168302) Note that Laplace transform (see [å¤å˜å‡½æ•°](https://shi200005.github.io/2022/02/15/Complex-Functions/#%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A2)) was applied to solve it.

  The [exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution), [Erlang distribution](https://en.wikipedia.org/wiki/Erlang_distribution), and [chi-squared distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution) (encountered in [æ•°ç†ç»Ÿè®¡](https://shi200005.github.io/2022/02/18/Statistics/#%E6%A0%B7%E6%9C%AC%E5%8F%8A%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83)) are special cases of the [gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution).

- å·²çŸ¥ä¸€ä¸ªéšæœºå˜é‡ä¸ºæŒ‡æ•°åˆ†å¸ƒï¼Œä¸å¦ä¸€ä¸ªéšæœºå˜é‡çš„å’Œä¸ºæŒ‡æ•°åˆ†å¸ƒï¼Œå¦ä¸€ä¸ªéšæœºå˜é‡æ˜¯æŒ‡æ•°åˆ†å¸ƒå—ï¼Ÿ**ä¸æ˜¯**ã€‚è§ Thomas & Cover Elements of Information Theory ä¹ é¢˜ 9.3ã€‚

- æ­£æ€ + æ­£æ€è¿˜æ˜¯æ­£æ€ï¼ŒæœŸæœ›ä¸ºæœŸæœ›å’Œï¼Œæ–¹å·®ä¸ºæ–¹å·®å’Œï¼Œè¯æ˜ï¼š[Sum of normally distributed random variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables)ã€‚

å¦‚æœä¸¤ä¸ªæ­£æ€åˆ†å¸ƒçš„éšæœºå˜é‡æœ‰ç›¸å…³æ€§ï¼Œåˆ™å®ƒä»¬çš„å’Œè¿˜æ˜¯é«˜æ–¯åˆ†å¸ƒï¼ŒæœŸæœ›ä¸ºç›¸åŠ ï¼Œæ–¹å·®å¹¶éç®€å•ç›¸åŠ ã€‚æ¨å¯¼è§ä¸Šé¢çš„é“¾æ¥ã€‚

### å¸¸è§åˆ†å¸ƒçš„å…³ç³»

![](/images/blog/Probability_Distribution.jpg)

æ³Šæ¾åˆ†å¸ƒï¼ˆå‚æ•° $$\lambda$$ï¼‰è¢«ä¼¯åŠªåˆ©æŒ‘é€‰ï¼ˆæ¦‚ç‡ $$p$$ï¼‰åï¼Œä»ä¸ºæ³Šæ¾åˆ†å¸ƒï¼ˆå‚æ•° $$p\lambda$$ï¼‰ã€‚è¯æ˜ä»ç•¥ã€‚

## å¤šç»´éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ

### è”åˆåˆ†å¸ƒ

ä»¥äºŒç»´éšæœºå˜é‡ä¸ºä¾‹ $$F(x,y)=P\{(Xâ‰¤x)âˆ©(Yâ‰¤y)\}=P\{Xâ‰¤x, Yâ‰¤y\}$$
 - è¿ç»­å‹éšæœºå˜é‡è”åˆæ¦‚ç‡å¯†åº¦ $$\displaystyle F(x,y)=\int_{-âˆ}^y{f(u,v)dudv}$$, $$\displaystyle\frac{\partial^2{F(x,y)}}{\partial x\partial y}=f(x,y)$$â€‹.

#### Multivariate Gaussian Distribution

$$G(\mathbf {x})=\displaystyle (2\pi )^{-k/2}\det({\boldsymbol {\Sigma }})^{-1/2}\,\exp \left(-{\frac {1}{2}}(\mathbf {x} -{\boldsymbol {\mu }})^{\!{\mathsf {T}}}{\boldsymbol {\Sigma }}^{-1}(\mathbf {x} -{\boldsymbol {\mu }})\right)$$

å…¶ä¸­ $$\mathbf {x}$$ æ˜¯ $$k$$ ç»´éšæœºå˜é‡æ‘æˆçš„ä¸€ä¸ªåˆ—å‘é‡ã€‚é—®é¢˜ï¼šä¸ºä»€ä¹ˆæŒ‡æ•°éƒ¨åˆ†å–åæ–¹å·®çŸ©é˜µçš„é€†ï¼Ÿå…¶å®ä¸ç”¨å¤ªç¹ççš„æ•°å­¦æ¨å¯¼â€”â€”Hint: åæ–¹å·®çŸ©é˜µæ˜¯å®å¯¹ç§°çŸ©é˜µï¼Œèƒ½å¤ŸåˆåŒå¯¹è§’åŒ–ï¼ˆ[çº¿æ€§ä»£æ•°](https://shi200005.github.io/2021/09/30/Linear-Algebra/)ï¼‰ï¼Œç„¶åè¯·è‡ªè¡Œå¯¹éšæœºå˜é‡è¿›è¡ŒåŸºå˜æ¢ï¼Œä½ ä¼šæ˜ç™½ã€‚

### è¾¹ç¼˜åˆ†å¸ƒ

ä»¥äºŒç»´éšæœºå˜é‡ä¸ºä¾‹ $$(X,Y)$$ å…³äº $$X$$ çš„è¾¹ç¼˜åˆ†å¸ƒå‡½æ•°ï¼š$$P\{Xâ‰¤x\}=P\{Xâ‰¤x,y<âˆ\}=F(x,âˆ)=F_X(x)$$

å·²çŸ¥è”åˆåˆ†å¸ƒå¯å¯¼å‡ºéšæœºå˜é‡å„è‡ªçš„æ¡ä»¶åˆ†å¸ƒï¼Œå·²çŸ¥å„è‡ªçš„æ¡ä»¶åˆ†å¸ƒä¸èƒ½å¯¼å‡ºå®ƒä»¬çš„è”åˆåˆ†å¸ƒã€‚

è¿ç»­å‹éšæœºå˜é‡è¾¹ç¼˜æ¦‚ç‡å¯†åº¦ï¼š$$\displaystyle f_X(x)=\int_{-âˆ}^{âˆ}{f(x,y)dy}$$.

### æ¡ä»¶åˆ†å¸ƒ

$$p(y\vert x)=\displaystyle\frac{p(x,y)}{p(x)}$$.

### ç›¸äº’ç‹¬ç«‹ä¸ä¸¤ä¸¤ç‹¬ç«‹

ä¸‰ä¸ªéšæœºå˜é‡**ç›¸äº’ç‹¬ç«‹**èƒ½æ¨å‡ºä¸‰å¯¹éšæœºå˜é‡**ä¸¤ä¸¤ç‹¬ç«‹**ï¼Œåä¹‹æ¨ä¸å‡ºã€‚ç¤ºä¾‹è§ä¸‹å›¾ï¼Œ$$Z=X\oplus Y$$ï¼Œå³å¸ƒå°”ä»£æ•°é‡Œçš„ XOR è¿ç®—ã€‚è¿™ä¸€ç‚¹åœ¨é€šä¿¡åŠ å¯†å’Œè§£ç ä¸­æœ‰åº”ç”¨ï¼ˆè¯¦è§ [(En) Information Theory](https://shi200005.github.io/2023/10/03/Information-Theory/)ï¼‰ã€‚

![Probability_Pairwise](\images\blog\Probability_Pairwise.jpg)

### å¤åˆå½¢å¼

$$p((z\vert y)\vert x)$$ å…¶å®å°±æ˜¯ $$p(z\vert(x,y))$$. [(En) Information Theory](https://shi200005.github.io/2023/10/03/Information-Theory/) ä¼šç”¨ã€‚

## éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾

### æœŸæœ›

$$E(X+Y)=E(X)+E(Y)$$ æ€»æˆç«‹

$$E(XY)=E(X)E(Y)$$ åœ¨ $$X,Y$$ ç›¸äº’ç‹¬ç«‹æ—¶æˆç«‹ã€‚

### æ–¹å·®

å®šä¹‰ï¼š$$D(X)=Var(X)=E\{[(X)-E(X)]^2\}$$.

- P.S. å¹¶ä¸æ˜¯æ‰€æœ‰æ¦‚ç‡åˆ†å¸ƒéƒ½å­˜åœ¨æœ‰é™çš„æ–¹å·®ï¼ä¾‹å¦‚ $$\displaystyle P(x)=\frac{1}{\pi}\frac{\gamma}{(x-a^2)+\gamma^2}(-\infty<x<\infty)$$ï¼Œä¹Ÿç§°ä¸º Lorentz-Cauchy distribution å°±æ²¡æœ‰ã€‚

### å¸¸è§ç¦»æ•£å‹éšæœºå˜é‡æœŸæœ›ä¸æ–¹å·®çš„æ±‚è§£

ã€GRAD-UPDATEã€‘ç¦»æ•£éšæœºå˜é‡åˆ†å¸ƒä¸­çš„äºŒé¡¹åˆ†å¸ƒã€æ³Šæ¾åˆ†å¸ƒå’Œå‡ ä½•åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®å¦‚ä½•è®¡ç®—ï¼Ÿåœ¨ UofT PHY2108 è¯¾ä¸Šï¼ŒProf Anton Zilman ä»‹ç»äº†ç”¨ **Generating function** è®¡ç®—äºŒé¡¹åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ã€‚è¿™ä¸ªæ–¹æ³•ä¹Ÿé€‚ç”¨äºè®¡ç®—æ³Šæ¾åˆ†å¸ƒå’Œå‡ ä½•åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ã€‚å…·ä½“è¿‡ç¨‹è§ [Generating Functions](https://shi200005.github.io/download_file/Probability_Generating_Functions.pdf)ã€‚

### Law of total expectation & law of total variance 

(using conditional probability of different cases)

[$$E(X)=E(E(X\vert Y))$$](https://en.wikipedia.org/wiki/Law_of_total_expectation) i.e. lives of light bulbs from different factories

[$$Var(X)=E[Var(X\vert Y)]+Var(E[X\vert Y])$$](https://en.wikipedia.org/wiki/Law_of_total_variance)

Related work in systems biology

- Huh, D., & Paulsson, J. (2011). Random partitioning of molecules at cell division. *Proceedings of the National Academy of Sciences*, *108*(36), 15004-15009. [online](https://www.pnas.org/doi/full/10.1073/pnas.1013171108)
- Hilfinger, A., & Paulsson, J. (2011). Separating intrinsic from extrinsic fluctuations in dynamic biological systems. *Proceedings of the National Academy of Sciences*, *108*(29), 12167-12172. [online](https://www.pnas.org/doi/10.1073/pnas.1018832108)

### é™„å½•

| ç§ç±» | åˆ†å¸ƒ                                                                                      | æ•°å­¦æœŸæœ› | æ–¹å·®           |
| ç¦»æ•£ | äºŒé¡¹åˆ†å¸ƒ $$P(X=m)=\binom{N}{m}p^m(1-p)^{N-m}$$, $$X\in \mathbb{N_0}$$ | $$np$$           | $$np(1-p)$$ |
| ç¦»æ•£ | æ³Šæ¾åˆ†å¸ƒ $$P(X=m)=\frac{x^m e^{-\lambda}}{m!}$$ , $$X\in \mathbb{N_0}$$                       | $$Î»$$              | $$Î»$$                |
| ç¦»æ•£ | å‡ ä½•åˆ†å¸ƒ $$P(X=m)=(1-p)^{k-1}p$$, $$X\in \mathbb{N_0}$$             | $$\frac{1}{p}$$             | $$\frac{1-p}{p^2}$$           |
| è¿ç»­ | å‡åŒ€åˆ†å¸ƒ $$f(X=x)=\frac{1}{b-a}$$, $$X\in[a,b]$$                          | $$\frac{a+b}{2}$$         |$$\frac{(b-a)^2}{12}$$         |
| è¿ç»­ | æŒ‡æ•°åˆ†å¸ƒ  $$f(X=x)=\lambda e^{-\lambda x}$$ ,$$X\in[0,\infty)$$                  | $$\frac{1}{Î¸}$$              | $$\frac{1}{Î¸^2}$$              |
| è¿ç»­ | æ­£æ€åˆ†å¸ƒ $$f(X=x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$$, $$X\in(-\infty,\infty)$$  | $$Î¼$$              | $$Ïƒ^2$$              |

- æ³Šæ¾åˆ†å¸ƒï¼ŒçœŸæ˜¯ä¸ªå¯çˆ±çš„åˆ†å¸ƒã€‚æ³•è¯­å°çŸ¥è¯†ï¼šPoisson åœ¨æ³•è¯­é‡Œæ˜¯ğŸŸçš„æ„æ€ã€‚

## éšæœºå˜é‡çš„ fancy æ•°å­—ç‰¹å¾

å“ˆå“ˆï¼Œè¿™ç¯‡æˆ‘æœ¬æ¥æƒ³é¿å…è¶…å‡ºæˆ‘ä»¬æœ¬ç§‘è¯¾ç¨‹çš„èŒƒå›´ï¼Œå½“æ—¶æ­»è®°ç¡¬èƒŒä¸çŸ¥æ‰€äº‘çš„å‡ ä¸ªçŸ¥è¯†åœ¨å¼•å…¥ä¸€äº›æ•°å­¦ä¸Šå¹¶ä¸å›°éš¾çš„æ–°ä¸œè¥¿ä¹‹åå°±å˜å¾—å¾ˆ intuitive äº†ã€‚ä½•ä¹è€Œä¸ä»‹ç»ï¼Ÿ

### çŸ©

çŸ©è¿˜è›®é‡è¦çš„ï¼Œè‹±æ–‡å« momentã€‚

### The characteristic function

ï¼ˆè¡¥å……ï¼Œå‚è§ Kardar 2.2ï¼‰

å¯¹æ¦‚ç‡åˆ†å¸ƒåš**å‚…é‡Œå¶å˜æ¢**ï¼ˆå‚è§[å¤å˜å‡½æ•°](https://shi200005.github.io/2022/02/15/Complex-Functions/#%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2)ï¼‰å¾—åˆ° *the characteristic function* $$\displaystyle\tilde p(k)=\langle e^{-ikx}\rangle=\int dxp(x)e^{-ikx}$$ï¼Œthen moments of the PDF around any point $$x_0$$ can be generated by expanding $$e^{ikx_0}\tilde p(k)=\displaystyle\sum_{n=0}^\infty\frac{(-ik)^n}{n!}\langle(x-x_0)^n\rangle$$ã€‚

### The cumulant generating function

ï¼ˆè¡¥å……ï¼Œå‚è§ Kardar 2.2ï¼‰

is $$\ln\tilde p(k)$$ï¼Œæˆ‘ä»¬ç§°æŠŠè¿™ä¸ªä¸œè¥¿å¯¹ $$k$$ çš„å±•å¼€é¡¹æ˜¯ *cumulants*ï¼ŒKardar åœ¨ä¹¦ä¸­ç”¨ $$\langle x^n\rangle_c$$ è¡¨ç¤ºï¼Œvan Kampen åœ¨ä¹¦ä¸­ç”¨ $$\langle\langle x^n\rangle\rangle$$ è¡¨ç¤ºï¼Œç»´åŸºç™¾ç§‘ä¸­è¡¨ç¤ºä¸º $$\kappa_n(x)$$ã€‚Why do we care about cumulants? å¯¹äºå›¾å½¢æ³•èƒŒåçš„æ•°å­¦ï¼Œåœ¨ä¹¦ä¸­å›¾å½¢æ³•çš„åä¸€é¡µè§£é‡Šå¾—å¾ˆæ¸…æ¥šäº†ï¼Œä¸å†èµ˜è¿°ã€‚

> An important theorem allows easy computation of moments in terms of the cumulants: represent the $$n$$th cumulant graphically as a *connected cluster* of $$n$$ points. The $$m$$th moment is then obtained by summing all possible subdivisions of $$m$$ points into groupings of smaller (connected or disconnected clusters).
>
> ![Probability_Cumulant](\images\blog\Probability_Cumulant.JPG)

é«˜æ–¯åˆ†å¸ƒ $$\displaystyle p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp[-\frac{(x-\lambda)^2}{2\sigma^2}]$$ çš„ä¼˜è‰¯å“è´¨æ˜¯ $$\langle x\rangle_c=\lambda$$, $$\langle x^2\rangle_c=\sigma^2$$, $$\langle x^3\rangle_c=\langle x^4\rangle_c=...=0$$â€‹ï¼Œä¹Ÿå°±æ˜¯ä»»ä½•é˜¶ä¸­å¿ƒçŸ©éƒ½å¯ä»¥ç”¨å‡å€¼å’Œæ–¹å·®ä¸¤ä¸ªå‚æ•°è¡¨ç¤ºã€‚

å¾—åï¼šå¦‚æœéšæœºå˜é‡ $$x_i$$ ç›¸äº’ç‹¬ç«‹ï¼Œåˆ™ $$\displaystyle\kappa_n(\sum x_i)=\sum\kappa_n(x_i)$$ã€‚

ä¸Šé¢è¿™äº›å•å˜é‡ç»Ÿè®¡åˆ†å¸ƒå¯ä»¥è§£å†³ï¼ˆæ›´é«˜çº§çš„å»çœ‹[(En) Advanced Statistical Mechanics](https://shi200005.github.io/2023/05/07/Advanced-Statistical-Mechanics/)ï¼‰ï¼Œä¸‹é¢æ˜¯å¤šå˜é‡åˆ†å¸ƒçš„ç®€å•æ€§è´¨ã€‚

### åæ–¹å·®

å®šä¹‰ï¼š$$Cov(X,Y)=E\{[(X)-E(X)][(Y)-E(Y)]\}=E(XY)-E(X)E(Y)$$.

$$D(X+Y)=D(X)+D(Y)+2E\{[(X)-E(X)][(Y)-E(Y)]\}$$.

ç›¸å…³ç³»æ•°ï¼š$$\displaystyle Ï_{XY}=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}$$.(Term: [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).)

åæ–¹å·®æ—¶äºŒé˜¶æ··åˆä¸­å¿ƒçŸ©ã€‚åæ–¹å·®çŸ©é˜µçš„å®šä¹‰...æ€§è´¨ï¼š[è¯æ˜ï¼šåæ–¹å·®çŸ©é˜µæ˜¯åŠæ­£å®šçŸ©é˜µ](https://blog.csdn.net/qcyfred/article/details/71598815)ï¼Œå‚è§[çº¿æ€§ä»£æ•°](https://shi200005.github.io/2021/09/30/Linear-Algebra/#%E5%AE%9E%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5)ã€‚

### ç›¸å…³ä¸ç‹¬ç«‹

ç›¸å…³ç³»æ•°æè¿°çš„æ˜¯**çº¿æ€§**ç›¸å…³ç¨‹åº¦ã€‚æ€§è´¨æ˜¯$$\vertÏ_{XY}\vertâ‰¤1$$ï¼Œå½“ä¸”ä»…å½“ $$Y=a+bX$$ æ—¶å–ç­‰å·ã€‚è‹¥$$X,Y$$ ç‹¬ç«‹ï¼Œåˆ™ç›¸å…³ç³»æ•°ä¸€å®šä¸ºé›¶ï¼›è‹¥ç›¸å…³ç³»æ•°ä¸ºé›¶ï¼Œä¸ä¸€å®šç‹¬ç«‹ã€‚æ„Ÿè§‰è¿™ä¸ªåå­—æœ‰ç‚¹è¯¯å¯¼æ€§ã€‚

è¡¥å……ï¼šæ¦‚å¿µï¼š*statistical independence* å’Œ *uncorrelated*ã€‚

uncorrelated: $$Ï_{XY}=0$$ (weaker than *statistical independence*)

statistical independence: expressed by any one of the three criteria:

1. All moments factorize: $$\langle X_1^{m_1}X_2^{m_2}\rangle=\langle X_1^{m_1}\rangle\langle X_2^{m_2}\rangle$$.
2. The characteristic function factorizes: $$G(k_1,k_2)=G_1(k_1)G_2(k_2)$$.
3. The cumulants $$\langle\langle X_1^{m_1}X_2^{m_2}\rangle \rangle$$ vanish when both $$m_1$$ and $$m_2$$ differ from zero. (the symbol for cumulants in van Kampen is different from that in Kardar, haha)

The reason why this property has a special name is that in many applications the first and second moments alone provide an adequate description (i.e. Gaussian distribution).

## å¤§æ•°å®šç†

### å¤§æ•°å®šç†

#### Chebyshev ä¸ç­‰å¼

*Markov's inequality*: $$X$$ non-negative, for every $$a>0$$, $$\displaystyle P[X\geq a]\geq\frac{E[X]}{a}$$.

*Chebyshev's inequality*: arbitrary $$X$$, $$Y=(X-E[X])^2$$ thus non-negative, plug in Markov inequality, $$P\{\vert X-Î¼\vert\geq Îµ\}\leq\frac{Ïƒ^2}{Îµ^2}$$, $$P\{\vert X-Î¼\vertï¼œÎµ\}â‰¥1-\frac{Ïƒ^2}{Îµ^2}$$. ç»™å‡ºäº†åœ¨éšæœºå˜é‡çš„åˆ†å¸ƒæœªçŸ¥ï¼Œè€ŒåªçŸ¥é“$$E(X)$$å’Œ$$D(X)$$çš„æƒ…å†µä¸‹ä¼°è®¡æ¦‚ç‡çš„ç•Œé™ã€‚

Note that æ ¸å¿ƒå…³ç³» $$\displaystyle D[\frac{1}{n}\sum_{k=1}^{n}{x_k}]=\frac{Ïƒ^2}{n}$$, which means $$\sigma$$~$$N^{-1/2}$$. Plug in *Chebyshev's inequality*, we have *For all $$\epsilon>0$$ and all $$\delta>0$$ there exists a positive integer $$n_0$$ such that for all $$n\geq n_0$$*, $$P[\vert M_n-m\vert\geq\epsilon]\leq\delta$$. 

æ ·æœ¬æ•°å¢åŠ ï¼Œæ ·æœ¬å‡å€¼ä¾æ¦‚ç‡æ”¶æ•›äºéšæœºå˜é‡å‡å€¼ã€‚$$P[\vert M_n-m\vert\geq\epsilon]\rightarrow 0$$ as $$n\rightarrow 0$$.

#### ä¼¯åŠªåˆ©å¤§æ•°å®šç†

é¢‘ç‡ä¾æ¦‚ç‡æ”¶æ•›äºæ¦‚ç‡ï¼ˆé¢‘ç‡ç¨³å®šæ€§ï¼‰ã€‚å½“è¯•éªŒæ¬¡æ•°å¾ˆå¤§æ—¶ï¼Œä¾¿å¯ä»¥ç”¨äº‹ä»¶çš„é¢‘ç‡æ¥ä»£æ›¿äº‹ä»¶çš„æ¦‚ç‡ã€‚

#### è¾›é’¦å®šç†

ä¸ Chebyshevï¼šä¸éœ€è¦æ–¹å·®å­˜åœ¨ã€‚

ä¸ä¼¯åŠªåˆ©ï¼šä¼¯åŠªåˆ©æ˜¯è¾›é’¦çš„ç‰¹æ®Šæƒ…å†µã€‚

æˆ‘æ ¹æœ¬çœ‹ä¸æ‡‚è¿™æ˜¯åœ¨è¯´ä»€ä¹ˆã€‚

### ä¸­å¿ƒæé™å®šç†
#### ç‹¬ç«‹åŒåˆ†å¸ƒçš„ä¸­å¿ƒæé™å®šç†

ç‹¬ç«‹åŒåˆ†å¸ƒçš„ä¸­å¿ƒæé™å®šç†ï¼š$$X$$~$$N(Î¼,Ïƒ^2)$$  ->  $$\bar{X}$$~$$N(Î¼,\frac{Ïƒ^2}{n})$$ ï¼ˆChebyshev é‚£é‡Œä¸æ˜¯å·²ç»ç”¨åˆ°äº†ï¼Œä¸ºå•¥å†æ:sweat_smile:ï¼‰

#### Lyapunov å®šç†

æ— è®ºå„ä¸ªéšæœºå˜é‡æœä»ä»€ä¹ˆåˆ†å¸ƒï¼Œåªè¦æ»¡è¶³å®šç†çš„æ¡ä»¶ï¼ˆthe cumulants of the individual random variables are finiteï¼‰ï¼Œé‚£ä¹ˆ**å®ƒä»¬çš„å’Œ**å½“ $$n$$ å¾ˆå¤§æ—¶ï¼Œå°±è¿‘ä¼¼åœ°æœä»**æ­£æ€åˆ†å¸ƒ**ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå…¶å® Kardar é‚£æœ¬ä¹¦çš„ç¬¬äºŒç« è®²å¾—å¾ˆæ¸…æ¥šäº†ã€‚

Sum $$X=\displaystyle\sum_{i=1}^{N}x_i$$, $$x_i$$ are i.i.d. from a distribution. We have $$\langle X^n\rangle_c=N\langle x^n\rangle_c$$. Construct $$\displaystyle y=\frac{X-N\langle x\rangle_c}{\sqrt N}$$, so $$\langle y^n\rangle_c\propto N^{1-n/2}$$. As $$N\rightarrow\infty$$, **only the second cumulant survives**, and the PDF for $$y$$ converges to the **normal distribution**, $$\displaystyle\lim_{Nâ†’\infty}p(y=\frac{\displaystyle\sum_{i=1}^N x_i-N\langle x \rangle_c}{\sqrt N})=\frac{1}{\sqrt{2\pi\langle x^2\rangle_c}}exp(-\frac{y^2}{2\langle x^2\rangle_c})$$. 

#### De Moiver-Laplace å®šç† 

æ˜¯ä¸­å¿ƒæé™å®šç†çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µã€‚æ ·æœ¬å®¹é‡å¾ˆå¤§æ—¶ï¼Œç‹¬ç«‹å®éªŒçš„æˆåŠŸæ¦‚ç‡æ¥è¿‘ $$1/2$$ æ—¶ï¼ŒæˆåŠŸæ¦‚ç‡ä¸º**äºŒé¡¹åˆ†å¸ƒ**ï¼Œä¹Ÿå°±æ˜¯æ‰€æœ‰æˆåŠŸäº‹ä»¶çš„åŠ å’Œï¼Œå¯ä»¥é€šè¿‡**æ–¯ç‰¹æ—å…¬å¼**ï¼ˆè§[ç»Ÿè®¡ç‰©ç†](https://shi200005.github.io/2022/09/10/Statistical-Mechanics/)ï¼‰å¾—åˆ°**æ­£æ€åˆ†å¸ƒ**å½¢å¼ï¼Œè¯¦è§[éšæœºè¿‡ç¨‹](https://shi200005.github.io/2022/10/28/Stochastic-Processes/)ã€‚
